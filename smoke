# host-level sanity
nvidia-smi
nvidia-smi -L
nvidia-smi topo -m

# inside your whisper venv (use WHISPER_PY)
WHISPER_PY=/opt/whisper-venv/bin/python3
$WHISPER_PY - <<'PY'
import torch,sys
print("torch:", getattr(torch,"__version__","missing"))
print("torch.version.cuda:", getattr(torch.version,"cuda",None))
print("cuda_available:", torch.cuda.is_available())
print("device_count:", torch.cuda.device_count())
if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        print(i, torch.cuda.get_device_name(i))
PY

